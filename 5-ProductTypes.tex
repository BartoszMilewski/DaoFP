\documentclass[DaoFP]{subfiles}
\begin{document}
\setcounter{chapter}{4}

\chapter{Product Types}

We can use sum types to enumerate possible values of a given type, but the encoding can be wasteful. We needed ten constructors just to encode numbers between zero and nine.
\begin{haskell}
data Digit = Zero | One | Two | Three | ... | Nine
\end{haskell}
But if we combine two digits into a single data structure, a two-digit decimal number, we'll able to encode a hundred numbers. Or, as Lao Tzu would say, with just four digits you can encode ten thousand numbers.

A data type that combines two types in this manner is called a product. Its defining quality is the elimination rule: there are two arrows coming from $A \times B$; one called ``$\text{fst}$'' goes to $A$, and another called ``$\text{snd}$'' goes to $B$. They are called \emph{projections}. They let us retrive $A$ and $B$ from the product $A \times B$.

\[
 \begin{tikzcd}
& A \times B
 \arrow[dl,  "\text{fst}"]
 \arrow[dr,   "\text{snd}"']
\\
A && B
 \end{tikzcd}
\]

Suppose that somebody gave you an element of a product, that is an arrow $h$ from the terminal object $1$ to $A \times B$. You can easily retrieve a pair of elements, just by using composition: an element of $A$ given by 
\[a = \text{fst} \circ h \]
and an element of $B$ given by
\[b = \text{snd} \circ h \]

\[
 \begin{tikzcd}
 & 1
\arrow[d, dashed, "h"]
 \arrow[ddl, bend right, "a"']
 \arrow[ddr, bend left, "b"]
\\
&A \times B
 \arrow[dl,  "\text{fst}"]
 \arrow[dr,   "\text{snd}"']
\\
A && B
 \end{tikzcd}
\]

In fact, given an arrow from an arbitrary object $C$ to $A \times B$, we can define, by composition, a pair of arrows $f \colon C \to A$ and $g \colon C \to B$

\[
 \begin{tikzcd}
 & C
\arrow[d, dashed, "h"]
 \arrow[ddl, bend right, "f"']
 \arrow[ddr, bend left, "g"]
\\
&A \times B
 \arrow[dl,  "\text{fst}"]
  \arrow[dr,   "\text{snd}"']
\\
A && B
 \end{tikzcd}
\]

As we did before with the sum type, we can turn this idea around, and use this diagram to define the product type: A pair of functions $f$ and $g$ should be in one-to-one correspondence with a \emph{mapping in} from $C$ to $A \times B$. This is the \emph{introduction} rule for the product.

In particular, the mapping out of the terminal object is used in Haskell to define a product type. Given two elements, \hask{a :: A} and \hask{b :: B}, we construct the product 

\begin{haskell}
(a, b) :: (A, B)
\end{haskell}
The built-in syntax for products is just that: a pair of parentheses and a comma in between. It works both for defining the product of two types \hask{(A, B)} and the data constructor \hask{(a, b)} that takes two elements and pairs them together. 

We should never lose sight of the purpose of programming: to decompose complex problems into a series of simpler ones. We see it again in the definition of the product. Whenever we have to construct a mapping \emph{into} the product, we decompose it into two smaller tasks of constructing a pair of functions, each mapping into one of the components of the product. This is as simple as saying that, in order to implement a function that returns a pair of values, it's enough to implement two functions, each returning one of the elements of the pair.

\subsection{Logic}

In logic, a product type corresponds to logical conjunction. In order to prove $A \times B$ ($A$ and $B$), you need to provide the proofs of \emph{both} $A$ and $B$. These are the arrows targeting $A$ and $B$. The elimination rule says that if you have a proof of $A \times B$, then you can get the proof of $A$ (through $\text{fst}$) and the proof of $B$ (through $\text{snd}$).

\subsection{Tuples and Records}

As Lao Tzu would say, a product of ten thousand objects is just an object with ten thousand projections. 

We can form such products in Haskell using the tuple notation. For instance, a product of three types is written as \hask{(A, B, C)}. A term of this type can be constructed from three elements: \hask{(a, b, c)}. 

In what mathematicians call ``abuse of notation'', a product of zero types is written as \hask{()}, an empty tuple, which happens to be the same as the terminal object, or unit type. This is because the product behaves very much like multiplication of numbers, with the terminal object playing the role of unit.

In Haskell, rather than defining separate projections for all tuples, we use the pattern-matching syntax. For instance, to extract the third component from a triple we would write

\begin{haskell}
thrd :: (a, b, c) -> c
thrd (_, _, c) = c
\end{haskell}
We use wildcards for the components that we want to ignore.

Lao Tzu said that ``Naming is the origin of all particular things.'' In programming, keeping track of the meaning of the components of a particular tuple is difficult without giving them names. Record syntax allows us to give names to projections. This is the definition of a product written in record style:
\begin{haskell}
data Product a b = Pair { fst :: a, snd :: b }
\end{haskell}
\hask{Pair} is the data constructor and \hask{fst} and \hask{snd} are the projections. 

This is how it could be used to declare and initialize a particular pair:
\begin{haskell}
ic :: Product Int Char
ic = Pair 10 'A'
\end{haskell}

\section{Cartesian Category}

In Haskell, we can define a product of any two types. A category in which all products exist, and the terminal object exists, is called \emph{cartesian}. 

\subsection{Tuple Arithmetic}

The identities satisfied by the product can be derived using the mapping-in property. For instance, to show that $A \times B \cong B \times A$ consider the following two diagrams:

\[
 \begin{tikzcd}
 &X
 \arrow[d, dashed, "h"]
 \arrow[ddl, bend right, "f"']
 \arrow[ddr, bend left, "g"]
 \\
 & A \times B
  \arrow[dl,  "\text{fst}"]
 \arrow[dr,   "\text{snd}"']
 \\
A && B
 \end{tikzcd}
 \qquad
 \begin{tikzcd}
 &X
 \arrow[d, dashed, "h'"]
 \arrow[ddl, bend right, "g"']
 \arrow[ddr, bend left, "f"]
 \\
 & B \times A
  \arrow[dl,  "\text{fst}"]
 \arrow[dr,   "\text{snd}"']
\\
B && A
  \end{tikzcd}
\]

They show that, for any object $X$ the arrows to $A \times B$ are in one-to-one correspondence with arrows to $B \times A$. This is because each of these arrows is determined by the same pair $f$ and $g$. 

You can check that the naturality condition is satisfied because, when you shift the perspective using $k \colon X' \to X$, all arrows originating in $X$ are shifted by pre-composition $(- \circ k)$.

In Haskell, this isomorphism can be implemented as a function which is its own inverse:
\begin{haskell}
swap :: (a, b) -> (b, a)
swap x = (snd x, fst x)
\end{haskell}
This is the same function written using pattern matching:
\begin{haskell}
swap (x, y) = (y, x)
\end{haskell}

It's important to keep in mind that the product is symmetric only ``up to isomorphism.'' It doesn't mean that swapping the order of pairs won't change the behavior of a program. Symmetry means that the information content of a swapped pair is the same, but access to it needs to be modified.

Here's the diagram that can be used to prove that the terminal object is the unit of the product, $1 \times A \cong A$.
\[
 \begin{tikzcd}
 &X
 \arrow[d, dashed, "h"]
 \arrow[ddl, bend right, "!"']
 \arrow[ddr, bend left, "f"]
 \\
 & 1 \times A
  \arrow[dl,  "\text{fst}"]
 \arrow[dr,   "\text{snd}"']
 \\
1 && A
 \end{tikzcd}
 \qquad
 \begin{tikzcd}
 X
 \arrow[dd, "f"]
\\
\\
A
  \end{tikzcd}
\]
The unique arrow from $X$ to $1$ is called $!$ (pronounced, \emph{bang}). Because of its uniqueness, the mapping-in, $h$, is totally determined by $f$.


Here are some other isomorphisms written in Haskell (without proofs of having the inverse). This is associativity:
\begin{haskell}
assoc :: (a, (b, c)) -> ((a, b), c)
assoc (a, (b, c)) = ((a, b), c)
\end{haskell}
And this is the left unit
\begin{haskell}
lunit :: ((), a) -> a
lunit (_, a) = a
\end{haskell}

\begin{exercise}
Show that the bijection in the proof of left unit is natural. Hint, change focus using an arrow $g \colon A \to B$.
\end{exercise}

\begin{exercise}
Construct an arrow 
\[ h \colon B + A \times B \to (1 + A) \times B \]
Is this arrow unique?

Hint: It's a mapping into a product, so it's given by a pair of arrow. These arrows, in turn, map out of a sum, so each is given by a pair of arrows. 

Hint: The mapping $B \to 1 + A$ is given by $(\text{Left} \, \circ \, !)$
\end{exercise}

\begin{exercise}
Re-do the previous exercise, this time treating $h$ as a mapping \emph{out} of a sum. 
\end{exercise}

\begin{exercise}
Implement a Haskell function \hask{maybeAB :: Either b (a, b) -> (Maybe a, b)}. Is this function uniquely defined by its type signature or is there some leeway?
\end{exercise}

\subsection{Functoriality}

Suppose that we have arrows that map $A$ and $B$ to some $A'$ and $B'$:
\[f \colon A \to A' \]
\[g \colon B \to B'\]
The composition of these arrows with the projections $\text{fst}$ and $\text{snd}$, respectively, can be used to define the mapping of the products:

\[
 \begin{tikzcd}
 & A \times B
\arrow[d, dashed, "h"]
 \arrow[dl,  "\text{fst}"']
 \arrow[dr,   "\text{snd}"]
\\
A
\arrow[d, "f"']
&A' \times B'
 \arrow[dl,  "\text{fst}"]
  \arrow[dr,   "\text{snd}"']
& B
\arrow[d, "g"]
\\
A' && B'
 \end{tikzcd}
\]

This property of the product is called \emph{functoriality}. You can imagine it as allowing you to transform the two objects \emph{inside} the product. 

\subsection{Symmetric Monoidal Category}

We have seen that the product satisfies these simple rules:
\[A \times 1 \cong A\]
\[A \times B \cong B \times A \]
\[(A \times B) \times C \cong A \times (B \times C) \]
and is functorial. 

A category in which an operation with these properties is defined is called \emph{symmetric monoidal}. We've seen this structure before when working with sums and the initial object. 

A category can have multiple monoidal structures at the same time. When you don't want to name your monoidal structure, you replace the plus sign or the product sign with a tensor sign, and the neutral element with the letter $I$. The rules of a symmetric monoidal category can then be written as:
\[A \otimes I \cong A\]
\[A \otimes B \cong B \otimes A \]
\[(A \otimes B) \otimes C \cong A \otimes (B \otimes C) \]

\section{Duality}

When a child sees an arrow, it knows which end points at the source, and which points at the target
\[A \to B \]
But maybe this is just a pre-conception. Would the Universe be very different if we called $B$ the source and $A$ the target? 

We would still be able to compose this arrow with this one
\[B \to C\]
whose ``target'' $B$ is the same as the same as the ``source'' of $A \to B$, and the result would still be an arrow 
\[A \to C\]
 only now we would say that it goes from $C$ to $A$.

In this dual Universe, the object that we call ``initial'' would be called ``terminal,'' because it's the ``target'' of unique arrows coming from all objects. Conversely, the terminal object would be called initial.

Now consider this diagram that we used to define the sum object:
\[
 \begin{tikzcd}
 A
 \arrow[dr,  bend left, "\text{Left}"']
 \arrow[ddr, bend right, "f"']
 && B
 \arrow[dl, bend right, "\text{Right}"]
 \arrow[ddl, bend left, "g"]
 \\
&A + B
\arrow[d, dashed, "h"]
\\
& C
 \end{tikzcd}
\]
In the new interpretation, the arrow $h$ maps ``from'' an arbitrary object $C$ ``to'' the object we call $A + B$. This arrow is uniquely defined by a pair of arrows $(f, g)$ whose ``source'' is $C$. If we rename $\text{Left}$ to $\text{fst}$ and $\text{Right}$ to $\text{snd}$, we will get  the defining diagram for a product. 

A product is the sum with arrows reversed. 

Conversely, a sum is the product with arrows reversed. 

\medskip

Every construction in category theory has its dual.

\medskip

If the direction of arrows is just a matter of interpretation, then what makes sum types so different from product types in programming? The difference goes back to one assumption we made at the start: There are no incoming arrows to the initial object (other than the identity arrow). This is in contrast with the terminal object having lots of outgoing arrows, which we used to define (global) elements. In fact, we assume that every object of interest has elements, and the ones that don't are isomorphic to \hask{Void}. 

We'll see an even deeper difference when we talk about function types.

\end{document}